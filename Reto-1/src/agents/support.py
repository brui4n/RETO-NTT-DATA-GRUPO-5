from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from src.core.state import TicketState
from src.core.llm import get_llm

def support_node(state: TicketState) -> dict:
    """
    Genera respuestas sugeridas para los usuarios finales o instrucciones para el t√©cnico.
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system", 
         "Eres la Inteligencia Artificial del sistema ITSM. "
         "Tu trabajo es leer la clasificaci√≥n y prioridad de un ticket y generar un 'AI Response' amigable y MUY BIEN estructurado.\n"
         "Instrucciones:\n"
         "- Justifica brevemente por qu√© se dio esa prioridad/tipo basado en el reporte.\n"
         "- Sugiere de 1 a 3 pasos t√©cnicos concretos de resoluci√≥n. ES OBLIGATORIO usar formato Markdown con dobles saltos de l√≠nea reales entre p√°rrafos y listas numeradas o vi√±etas para no amontonar el texto.\n"
         "- Si es prioridad 'critical' o 'high', menci√≥nalo expl√≠citamente y usa emojis de alerta üö®/üî¥.\n"
         "- S√© directo, emp√°tico y profesional."
        ),
        ("human", 
         "**T√≠tulo:** {title}\n"
         "**Ticket crudo:** {description}\n"
         "**Tipo:** {ticket_type}\n"
         "**Prioridad asignada:** {priority}\n\n"
         "Redacta tu respuesta en Markdown bien espaciado:"
        )
    ])
    
    # En lugar de with_structured_output generamos texto crudo (Markdown) directamente
    llm = get_llm(temperature=0.3)
    chain = prompt | llm | StrOutputParser()
    
    result = chain.invoke({
        "title": state.get("title", ""),
        "description": state["description"],
        "ticket_type": state.get("ticket_type", "N/A"),
        "priority": state.get("priority", "N/A")
    })
    
    return {
        "ai_response": result
    }
