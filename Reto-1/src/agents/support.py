from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from src.core.state import TicketState
from src.core.llm import get_llm

class SupportOutput(BaseModel):
    ai_response: str = Field(
        description="Respuesta profesional enviada al panel ITSM, engloba de forma educada una justificaci√≥n de la categor√≠a/prioridad asignada y una sugerencia de resoluci√≥n."
    )

def support_node(state: TicketState) -> dict:
    """
    Genera respuestas sugeridas para los usuarios finales o instrucciones para el t√©cnico.
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system", 
         "Eres la Inteligencia Artificial del sistema ITSM. "
         "Tu trabajo es leer la clasificaci√≥n y prioridad de un ticket y generar un 'AI Response' amigable y estructurado.\n"
         "Instrucciones:\n"
         "- Justifica brevemente por qu√© se dio esa prioridad/tipo basado en el reporte.\n"
         "- Sugiere de 1 a 3 pasos t√©cnicos concretos de resoluci√≥n para que el t√©cnico lo copie o el usuario lo aplique.\n"
         "- Si es prioridad 'critical' o 'high', menci√≥nalo expl√≠citamente y usa emojis de alerta üö®/üî¥.\n"
         "- S√© directo, emp√°tico y profesional."
        ),
        ("human", 
         "**T√≠tulo:** {title}\n"
         "**Ticket crudo:** {description}\n"
         "**Tipo:** {ticket_type}\n"
         "**Prioridad asignada:** {priority}\n\n"
         "Redacta el AI Response:"
        )
    ])
    
    llm = get_llm(temperature=0.3).with_structured_output(SupportOutput)
    chain = prompt | llm
    
    result: SupportOutput = chain.invoke({
        "title": state.get("title", ""),
        "description": state["description"],
        "ticket_type": state.get("ticket_type", "N/A"),
        "priority": state.get("priority", "N/A")
    })
    
    return {
        "ai_response": result.ai_response
    }
